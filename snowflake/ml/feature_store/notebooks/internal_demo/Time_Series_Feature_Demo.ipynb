{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ce01b3",
   "metadata": {},
   "source": [
    "Steps to run notebook:\n",
    "1. Create a conda env with python3.8 (Empty conda env)\n",
    "```\n",
    "conda create --name snowml python=3.8\n",
    "```\n",
    "2. Activate conda env\n",
    "```\n",
    "conda activate snowml\n",
    "```\n",
    "3. Install conda pkg\n",
    "```\n",
    "conda install snowflake-ml-python \n",
    "# or local build if there are changes in SnowML lib you need: bazel build //snowflake/ml:wheel\n",
    "# then do pip install {built pkg}\n",
    "```\n",
    "4. Install jupyter notebook\n",
    "```\n",
    "conda install jupyter\n",
    "```\n",
    "5. Start notebook\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160eb3e",
   "metadata": {},
   "source": [
    "## Feature Store Example with Time Series Features\n",
    "This notebook demonstrates advanced feature store usage with time series features. \n",
    "It will compute features from NY taxi trip data and demonstrate connections between training and prediction.\n",
    "The reference example by Databricks is here: https://docs.databricks.com/en/_extras/notebooks/source/machine-learning/feature-store-with-uc-taxi-example.html#feature-store/feature-store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f4de1",
   "metadata": {},
   "source": [
    "## Setup UI and Auto Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale cell width with the browser window to accommodate .show() commands for wider tables.\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3d0e0",
   "metadata": {},
   "source": [
    "#### [Optional 1] Import from local code repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11935b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Simplify reading from the local repository\n",
    "cwd=os.getcwd()\n",
    "REPO_PREFIX=\"snowflake/ml\"\n",
    "LOCAL_REPO_PATH=cwd[:cwd.find(REPO_PREFIX)].rstrip('/')\n",
    "\n",
    "if LOCAL_REPO_PATH not in sys.path:\n",
    "    print(f\"Adding {LOCAL_REPO_PATH} to system path\")\n",
    "    sys.path.append(LOCAL_REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd3a7d",
   "metadata": {},
   "source": [
    "#### [Optional 2] Import from installed snowflake-ml-python wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671378ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "conda_env = os.environ['CONDA_DEFAULT_ENV']\n",
    "import sys\n",
    "sys.path.append(f'/opt/homebrew/anaconda3/envs/{conda_env}/lib/python3.8/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5ac3a",
   "metadata": {},
   "source": [
    "## Prepare demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark.types import DateType, TimeType, _NumericType, TimestampType\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = session.table(\"SNOWML_FEATURE_STORE_TEST_DB.TEST_DATASET.yellow_tripdata_2016_01\")\n",
    "\n",
    "source_df = source_df.select(\n",
    "    [\n",
    "        \"TRIP_DISTANCE\", \n",
    "        \"FARE_AMOUNT\",\n",
    "        \"PASSENGER_COUNT\",\n",
    "        \"PULOCATIONID\",\n",
    "        \"DOLOCATIONID\",\n",
    "        F.cast(source_df.TPEP_PICKUP_DATETIME / 1000000, TimestampType()).alias(\"PICKUP_TS\"),\n",
    "        F.cast(source_df.TPEP_DROPOFF_DATETIME / 1000000, TimestampType()).alias(\"DROPOFF_TS\"),\n",
    "    ]).filter(\"DROPOFF_TS >= '2016-01-01 00:00:00' AND DROPOFF_TS < '2016-01-03 00:00:00'\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162799",
   "metadata": {},
   "source": [
    "## Create FeatureStore Client\n",
    "\n",
    "Let's first create a feature store client.\n",
    "\n",
    "We can pass in an existing database name, or a new database will be created upon the feature store initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_DB = \"FS_TIME_SERIES_EXAMPLE\"\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {DEMO_DB}\").collect()  # start from scratch\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DEMO_DB}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE WAREHOUSE PUBLIC WITH WAREHOUSE_SIZE='XSMALL'\").collect()\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=DEMO_DB, \n",
    "    name=\"AWESOME_FS\", \n",
    "    default_warehouse=\"PUBLIC\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052074",
   "metadata": {},
   "source": [
    "## Create and register new Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70609920",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pickup = Entity(name=\"trip_pickup\", join_keys=[\"PULOCATIONID\"])\n",
    "trip_dropoff = Entity(name=\"trip_dropoff\", join_keys=[\"DOLOCATIONID\"])\n",
    "fs.register_entity(trip_pickup)\n",
    "fs.register_entity(trip_dropoff)\n",
    "fs.list_entities().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4393",
   "metadata": {},
   "source": [
    "## Define feature pipeline\n",
    "We will compute a few time series features in the pipeline here.\n",
    "Before we have *__value based range between__* in SQL, we will use a work around to mimic the calculation (NOTE: the work around won't be very accurate on computing the time series value due to missing gap filling functionality, but it should be enough for a demo purpose)\n",
    "\n",
    "We will define two feature groups:\n",
    "1. pickup features\n",
    "    - Mean fare amount over the past 2 and 5 hours\n",
    "2. dropoff features\n",
    "    - Count of trips over the past 2 and 5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71729be3",
   "metadata": {},
   "source": [
    "### This is a UDF computing time window end\n",
    "We will later turn these into built in functions for feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\n",
    "    name=\"vec_window_end\",\n",
    "    is_permanent=True,\n",
    "    stage_location=session.get_session_stage(),\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    replace=True,\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) // np.timedelta64(1, 's') // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73742b89",
   "metadata": {},
   "source": [
    "### Define feature pipeline logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# NOTE: these time window calculations are approximates and are not handling time gaps\n",
    "\n",
    "def pre_aggregate_fn(df, ts_col, group_by_cols):\n",
    "    df = df.with_column(\"WINDOW_END\", F.call_udf(\"vec_window_end\", F.col(ts_col), \"15m\"))\n",
    "    df = df.group_by(group_by_cols + [\"WINDOW_END\"]).agg(\n",
    "            F.sum(\"FARE_AMOUNT\").alias(\"FARE_SUM_1_hr\"),\n",
    "            F.count(\"*\").alias(\"TRIP_COUNT_1_hr\")\n",
    "         )\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"PICKUP_TS\", [\"PULOCATIONID\"])\n",
    "    \n",
    "    window1 = Window.partition_by(\"PULOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"PULOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            \"SUM_FARE_2_hr\",\n",
    "            \"COUNT_TRIP_2hr\",\n",
    "            \"SUM_FARE_5_hr\",\n",
    "            \"COUNT_TRIP_5hr\",\n",
    "        ],\n",
    "        [\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window1),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1),\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window2),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2),\n",
    "        ]\n",
    "    ).select(\n",
    "        [\n",
    "            col(\"PULOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            (col(\"SUM_FARE_2_hr\") / col(\"COUNT_TRIP_2hr\")).alias(\"MEAN_FARE_2_hr\"),\n",
    "            (col(\"SUM_FARE_5_hr\") / col(\"COUNT_TRIP_5hr\")).alias(\"MEAN_FARE_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def dropoff_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"DROPOFF_TS\", [\"DOLOCATIONID\"])\n",
    "    window1 = Window.partition_by(\"DOLOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"DOLOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            col(\"DOLOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1).alias(\"COUNT_TRIP_2_hr\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2).alias(\"COUNT_TRIP_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "pickup_df = pickup_features_fn(source_df)\n",
    "pickup_df.show()\n",
    "\n",
    "dropoff_df = dropoff_features_fn(source_df)\n",
    "dropoff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46fa4f",
   "metadata": {},
   "source": [
    "## Create FeatureViews and materialize\n",
    "\n",
    "Once the FeatureView construction is done, we can materialize the FeatureView to the Snowflake backend and incremental maintenance will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fv = FeatureView(name=\"trip_pickup_time_series_features\", entities=[trip_pickup], feature_df=pickup_df, timestamp_col=\"ts\")\n",
    "pickup_fv = fs.register_feature_view(feature_view=pickup_fv, version=\"v1\", refresh_freq=\"1 minute\", block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8960b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_fv = FeatureView(name=\"trip_dropoff_time_series_features\", entities=[trip_dropoff], feature_df=dropoff_df, timestamp_col=\"ts\")\n",
    "fs.register_feature_view(feature_view=dropoff_fv, version=\"v1\", refresh_freq=\"1 minute\", block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009c81",
   "metadata": {},
   "source": [
    "## Explore FeatureViews\n",
    "We can easily discover what are the materialized FeatureViews and the corresponding features with *__fs.list_feature_views()__*. \n",
    "\n",
    "We can also apply filters based on Entity name or FeatureView names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.list_feature_views(entity_name=\"trip_pickup\").select([\"NAME\", \"VERSION\", \"ENTITIES\", \"FEATURE_DESC\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302cf23",
   "metadata": {},
   "source": [
    "## Generate training data and train a model\n",
    "The training data generation will lookup __point-in-time correct__ feature values and join with the spine dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_df = source_df.select([\"PULOCATIONID\", \"DOLOCATIONID\", \"PICKUP_TS\", \"FARE_AMOUNT\"])\n",
    "training_data = fs.generate_dataset(\n",
    "    spine_df=spine_df,\n",
    "    features=[pickup_fv, dropoff_fv],\n",
    "    materialized_table=\"yellow_tripdata_2016_01_training_data\",\n",
    "    spine_timestamp_col=\"PICKUP_TS\",\n",
    "    spine_label_cols = [\"FARE_AMOUNT\"]\n",
    ")\n",
    "\n",
    "training_data.df.show()\n",
    "training_data.df.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bced5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "training_pd = training_data.df.to_pandas()\n",
    "X = training_pd.drop([\"FARE_AMOUNT\", \"PICKUP_TS\"], axis=1)\n",
    "y = training_pd[\"FARE_AMOUNT\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "estimator = make_pipeline(imp, LinearRegression())\n",
    "\n",
    "reg = estimator.fit(X, y)\n",
    "r2_score = reg.score(X_test, y_test)\n",
    "print(r2_score * 100,'%')\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142c25c",
   "metadata": {},
   "source": [
    "## Log model with Model Registry\n",
    "We can log the model along with its training dataset metadata with model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import model_registry, artifact\n",
    "import time\n",
    "\n",
    "registry = model_registry.ModelRegistry(session=session, database_name=\"my_cool_registry\", create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caab287",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_ref = registry.log_artifact(\n",
    "    artifact_type=artifact.ArtifactType.DATASET,\n",
    "    artifact_name=\"MY_COOL_DATASET\",\n",
    "    artifact_spec=training_data.to_json(),\n",
    "    artifact_version=\"V1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"my_model_{time.time()}\"\n",
    "\n",
    "model_ref = registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=\"v1\",\n",
    "    model=estimator,\n",
    "    artifacts=[artifact_ref],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0581d",
   "metadata": {},
   "source": [
    "## Restore model and predict with latest features\n",
    "We retrieve the training dataset from registry then construct dataframe of latest feature values. Then we restore the model from registry. At last, we can predict with latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some source prediction data\n",
    "pred_df = training_pd.sample(3, random_state=996)[['PULOCATIONID', 'DOLOCATIONID', 'PICKUP_TS']]\n",
    "pred_df = session.create_dataframe(pred_df)\n",
    "pred_df = pred_df.select('PULOCATIONID', 'DOLOCATIONID', F.cast(pred_df.PICKUP_TS / 1000000, TimestampType()).alias('PICKUP_TS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich source prediction data with features\n",
    "from snowflake.ml.dataset.dataset import Dataset\n",
    "\n",
    "registered_artifact = registry.get_artifact(\n",
    "    artifact_ref.name, \n",
    "    artifact_ref.version)\n",
    "registered_dataset = Dataset.from_json(registered_artifact._spec, session)\n",
    "\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    spine_df=pred_df, \n",
    "    features=registered_dataset.load_features(), \n",
    "    spine_timestamp_col='PICKUP_TS'\n",
    ").drop(['PICKUP_TS']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = model_registry.ModelReference(\n",
    "    registry=registry, \n",
    "    model_name=model_name, \n",
    "    model_version=\"v1\"\n",
    ").load_model()\n",
    "\n",
    "pred = model_ref.predict(enriched_df)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad5af0",
   "metadata": {},
   "source": [
    "## DO NOT READ\n",
    "Below is a simple test for the window_end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ba589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "import datetime\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()\n",
    "\n",
    "udf_name = \"window_end\"\n",
    "    \n",
    "@F.pandas_udf(\n",
    "    name=udf_name,\n",
    "    replace=True,\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) // np.timedelta64(1, 's') // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')\n",
    "\n",
    "df = session.create_dataframe(\n",
    "    [\n",
    "        '2023-01-31 01:02:03.004',\n",
    "        '2023-01-31 01:14:59.999',\n",
    "        '2023-01-31 01:15:00.000',\n",
    "        '2023-01-31 01:15:00.004',\n",
    "        '2023-01-31 01:17:10.007',\n",
    "    ], \n",
    "    schema=['a']\n",
    ")\n",
    "df = df.select([F.to_timestamp(\"a\").alias(\"ts\")])\n",
    "\n",
    "df = df.select([\"TS\", F.call_udf(udf_name, F.col(\"TS\"), \"15m\").alias(\"window_end\")])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"select window_end(ts, '15m') from foobar\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
