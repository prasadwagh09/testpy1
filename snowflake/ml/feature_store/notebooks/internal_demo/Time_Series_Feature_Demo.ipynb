{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ce01b3",
   "metadata": {},
   "source": [
    "Steps to run notebook:\n",
    "1. Create a conda env with python3.8 (Empty conda env)\n",
    "```\n",
    "conda create --name snowml python=3.8\n",
    "```\n",
    "2. Activate conda env\n",
    "```\n",
    "conda activate snowml\n",
    "```\n",
    "3. Install conda pkg\n",
    "```\n",
    "conda install snowflake-ml-python \n",
    "# or local build if there are changes in SnowML lib you need: bazel build //snowflake/ml:wheel\n",
    "# then do pip install {built pkg}\n",
    "```\n",
    "4. Install jupyter notebook\n",
    "```\n",
    "conda install jupyter\n",
    "```\n",
    "5. Start notebook\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160eb3e",
   "metadata": {},
   "source": [
    "## Feature Store Example with Time Series Features\n",
    "This notebook demonstrates advanced feature store usage with time series features. \n",
    "It will compute features from NY taxi trip data and demonstrate connections between training and prediction.\n",
    "The reference example by Databricks is here: https://docs.databricks.com/en/_extras/notebooks/source/machine-learning/feature-store-with-uc-taxi-example.html#feature-store/feature-store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f4de1",
   "metadata": {},
   "source": [
    "## Setup UI and Auto Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da1a922d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Scale cell width with the browser window to accommodate .show() commands for wider tables.\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3d0e0",
   "metadata": {},
   "source": [
    "#### [Optional 1] Import from local code repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11935b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Simplify reading from the local repository\n",
    "cwd=os.getcwd()\n",
    "REPO_PREFIX=\"snowflake/ml\"\n",
    "LOCAL_REPO_PATH=cwd[:cwd.find(REPO_PREFIX)].rstrip('/')\n",
    "\n",
    "if LOCAL_REPO_PATH not in sys.path:\n",
    "    print(f\"Adding {LOCAL_REPO_PATH} to system path\")\n",
    "    sys.path.append(LOCAL_REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd3a7d",
   "metadata": {},
   "source": [
    "#### [Optional 2] Import from installed snowflake-ml-python wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "671378ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/tmp/snowml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5ac3a",
   "metadata": {},
   "source": [
    "## Prepare demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f39a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark.types import DateType, TimeType, _NumericType, TimestampType\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e665bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75bfcfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"TRIP_DISTANCE\"  |\"FARE_AMOUNT\"  |\"PASSENGER_COUNT\"  |\"PULOCATIONID\"  |\"DOLOCATIONID\"  |\"PICKUP_TS\"          |\"DROPOFF_TS\"         |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|3.2              |14.0           |1                  |48              |262             |2016-01-01 00:12:22  |2016-01-01 00:29:14  |\n",
      "|1.0              |9.5            |2                  |162             |48              |2016-01-01 00:41:31  |2016-01-01 00:55:10  |\n",
      "|0.9              |6.0            |1                  |246             |90              |2016-01-01 00:53:37  |2016-01-01 00:59:57  |\n",
      "|0.8              |5.0            |1                  |170             |162             |2016-01-01 00:13:28  |2016-01-01 00:18:07  |\n",
      "|1.8              |11.0           |1                  |161             |140             |2016-01-01 00:33:04  |2016-01-01 00:47:14  |\n",
      "|2.3              |11.0           |1                  |141             |137             |2016-01-01 00:49:47  |2016-01-01 01:04:44  |\n",
      "|13.8             |43.0           |1                  |100             |53              |2016-01-01 00:41:58  |2016-01-01 01:22:06  |\n",
      "|3.46             |20.0           |5                  |48              |79              |2016-01-01 00:25:28  |2016-01-01 00:55:46  |\n",
      "|0.83             |5.5            |4                  |79              |107             |2016-01-01 00:56:57  |2016-01-01 01:02:24  |\n",
      "|0.87             |7.0            |1                  |164             |164             |2016-01-01 00:10:08  |2016-01-01 00:23:05  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df = session.table(\"SNOWML_FEATURE_STORE_TEST_DB.TEST_DATASET.yellow_tripdata_2016_01\")\n",
    "\n",
    "source_df = source_df.select(\n",
    "    [\n",
    "        \"TRIP_DISTANCE\", \n",
    "        \"FARE_AMOUNT\",\n",
    "        \"PASSENGER_COUNT\",\n",
    "        \"PULOCATIONID\",\n",
    "        \"DOLOCATIONID\",\n",
    "        F.cast(source_df.TPEP_PICKUP_DATETIME / 1000000, TimestampType()).alias(\"PICKUP_TS\"),\n",
    "        F.cast(source_df.TPEP_DROPOFF_DATETIME / 1000000, TimestampType()).alias(\"DROPOFF_TS\"),\n",
    "    ]).filter(\"DROPOFF_TS >= '2016-01-01 00:00:00' AND DROPOFF_TS < '2016-01-03 00:00:00'\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162799",
   "metadata": {},
   "source": [
    "## Create FeatureStore Client\n",
    "\n",
    "Let's first create a feature store client.\n",
    "\n",
    "We can pass in an existing database name, or a new database will be created upon the feature store initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6c37a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_DB = \"FS_TIME_SERIES_EXAMPLE\"\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {DEMO_DB}\").collect()  # start from scratch\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DEMO_DB}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE WAREHOUSE PUBLIC WITH WAREHOUSE_SIZE='XSMALL'\").collect()\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=DEMO_DB, \n",
    "    name=\"AWESOME_FS\", \n",
    "    default_warehouse=\"PUBLIC\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052074",
   "metadata": {},
   "source": [
    "## Create and register new Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "70609920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>JOIN_KEYS</th>\n",
       "      <th>DESC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRIP_DROPOFF</td>\n",
       "      <td>DOLOCATIONID</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRIP_PICKUP</td>\n",
       "      <td>PULOCATIONID</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           NAME     JOIN_KEYS DESC\n",
       "0  TRIP_DROPOFF  DOLOCATIONID     \n",
       "1   TRIP_PICKUP  PULOCATIONID     "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_pickup = Entity(name=\"trip_pickup\", join_keys=[\"PULOCATIONID\"])\n",
    "trip_dropoff = Entity(name=\"trip_dropoff\", join_keys=[\"DOLOCATIONID\"])\n",
    "fs.register_entity(trip_pickup)\n",
    "fs.register_entity(trip_dropoff)\n",
    "fs.list_entities().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4393",
   "metadata": {},
   "source": [
    "## Define feature pipeline\n",
    "We will compute a few time series features in the pipeline here.\n",
    "Before we have *__value based range between__* in SQL, we will use a work around to mimic the calculation (NOTE: the work around won't be very accurate on computing the time series value due to missing gap filling functionality, but it should be enough for a demo purpose)\n",
    "\n",
    "We will define two feature groups:\n",
    "1. pickup features\n",
    "    - Mean fare amount over the past 2 and 5 hours\n",
    "2. dropoff features\n",
    "    - Count of trips over the past 2 and 5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71729be3",
   "metadata": {},
   "source": [
    "### This is a UDF computing time window end\n",
    "We will later turn these into built in functions for feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "995b4bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark.session:The version of package 'numpy' in the local environment is 1.24.4, which does not fit the criteria for the requirement 'numpy'. Your UDF might not work when the package version is different between the server and your local environment.\n",
      "WARNING:snowflake.snowpark.session:Package 'pytimeparse' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(\n",
    "    name=\"vec_window_end\",\n",
    "    is_permanent=True,\n",
    "    stage_location=session.get_session_stage(),\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    replace=True,\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) // np.timedelta64(1, 's') // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73742b89",
   "metadata": {},
   "source": [
    "### Define feature pipeline logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7d0c4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "|\"PULOCATIONID\"  |\"TS\"                 |\"MEAN_FARE_2_HR\"    |\"MEAN_FARE_5_HR\"    |\n",
      "----------------------------------------------------------------------------------\n",
      "|49              |2016-01-01 00:15:00  |7.0                 |7.0                 |\n",
      "|49              |2016-01-01 00:30:00  |9.3                 |9.3                 |\n",
      "|49              |2016-01-01 00:45:00  |9.409090909090908   |9.409090909090908   |\n",
      "|49              |2016-01-01 01:00:00  |12.296296296296296  |12.296296296296296  |\n",
      "|49              |2016-01-01 01:15:00  |13.540816326530612  |13.540816326530612  |\n",
      "|49              |2016-01-01 01:30:00  |13.27027027027027   |13.27027027027027   |\n",
      "|49              |2016-01-01 01:45:00  |13.145              |13.145              |\n",
      "|49              |2016-01-01 02:00:00  |13.007936507936508  |13.007936507936508  |\n",
      "|49              |2016-01-01 02:15:00  |13.00326797385621   |12.925806451612903  |\n",
      "|49              |2016-01-01 02:30:00  |13.258064516129032  |13.154450261780104  |\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "|\"DOLOCATIONID\"  |\"TS\"                 |\"COUNT_TRIP_2_HR\"  |\"COUNT_TRIP_5_HR\"  |\n",
      "--------------------------------------------------------------------------------\n",
      "|255             |2016-01-01 00:15:00  |2                  |2                  |\n",
      "|255             |2016-01-01 00:30:00  |24                 |24                 |\n",
      "|255             |2016-01-01 00:45:00  |70                 |70                 |\n",
      "|255             |2016-01-01 01:00:00  |118                |118                |\n",
      "|255             |2016-01-01 01:15:00  |162                |162                |\n",
      "|255             |2016-01-01 01:30:00  |221                |221                |\n",
      "|255             |2016-01-01 01:45:00  |272                |272                |\n",
      "|255             |2016-01-01 02:00:00  |333                |333                |\n",
      "|255             |2016-01-01 02:15:00  |398                |400                |\n",
      "|255             |2016-01-01 02:30:00  |441                |465                |\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# NOTE: these time window calculations are approximates and are not handling time gaps\n",
    "\n",
    "def pre_aggregate_fn(df, ts_col, group_by_cols):\n",
    "    df = df.with_column(\"WINDOW_END\", F.call_udf(\"vec_window_end\", F.col(ts_col), \"15m\"))\n",
    "    df = df.group_by(group_by_cols + [\"WINDOW_END\"]).agg(\n",
    "            F.sum(\"FARE_AMOUNT\").alias(\"FARE_SUM_1_hr\"),\n",
    "            F.count(\"*\").alias(\"TRIP_COUNT_1_hr\")\n",
    "         )\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"PICKUP_TS\", [\"PULOCATIONID\"])\n",
    "    \n",
    "    window1 = Window.partition_by(\"PULOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"PULOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            \"SUM_FARE_2_hr\",\n",
    "            \"COUNT_TRIP_2hr\",\n",
    "            \"SUM_FARE_5_hr\",\n",
    "            \"COUNT_TRIP_5hr\",\n",
    "        ],\n",
    "        [\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window1),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1),\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window2),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2),\n",
    "        ]\n",
    "    ).select(\n",
    "        [\n",
    "            col(\"PULOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            (col(\"SUM_FARE_2_hr\") / col(\"COUNT_TRIP_2hr\")).alias(\"MEAN_FARE_2_hr\"),\n",
    "            (col(\"SUM_FARE_5_hr\") / col(\"COUNT_TRIP_5hr\")).alias(\"MEAN_FARE_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def dropoff_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"DROPOFF_TS\", [\"DOLOCATIONID\"])\n",
    "    window1 = Window.partition_by(\"DOLOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"DOLOCATIONID\").order_by(col(\"WINDOW_END\").desc()).rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            col(\"DOLOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1).alias(\"COUNT_TRIP_2_hr\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2).alias(\"COUNT_TRIP_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "pickup_df = pickup_features_fn(source_df)\n",
    "pickup_df.show()\n",
    "\n",
    "dropoff_df = dropoff_features_fn(source_df)\n",
    "dropoff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46fa4f",
   "metadata": {},
   "source": [
    "## Create FeatureViews and materialize\n",
    "\n",
    "Once the FeatureView construction is done, we can materialize the FeatureView to the Snowflake backend and incremental maintenance will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f0cd2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fv = FeatureView(name=\"trip_pickup_time_series_features\", entities=[trip_pickup], feature_df=pickup_df, timestamp_col=\"ts\")\n",
    "pickup_fv = fs.register_feature_view(feature_view=pickup_fv, version=\"v1\", refresh_freq=\"1 minute\", block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8960b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureView(_name=TRIP_DROPOFF_TIME_SERIES_FEATURES, _entities=[Entity(name=TRIP_DROPOFF, join_keys=['DOLOCATIONID'], desc=)], _feature_df=<snowflake.snowpark.dataframe.DataFrame object at 0x2ae0f3c40>, _timestamp_col=TS, _desc=, _query=SELECT \"DOLOCATIONID\", \"WINDOW_END\" AS \"TS\", sum(\"TRIP_COUNT_1_HR\") OVER (PARTITION BY \"DOLOCATIONID\"  ORDER BY \"WINDOW_END\" DESC NULLS LAST  ROWS BETWEEN CURRENT ROW AND 7 FOLLOWING  ) AS \"COUNT_TRIP_2_HR\", sum(\"TRIP_COUNT_1_HR\") OVER (PARTITION BY \"DOLOCATIONID\"  ORDER BY \"WINDOW_END\" DESC NULLS LAST  ROWS BETWEEN CURRENT ROW AND 19 FOLLOWING  ) AS \"COUNT_TRIP_5_HR\" FROM ( SELECT \"DOLOCATIONID\", \"WINDOW_END\", sum(\"FARE_AMOUNT\") AS \"FARE_SUM_1_HR\", count(1) AS \"TRIP_COUNT_1_HR\" FROM ( SELECT \"TRIP_DISTANCE\", \"FARE_AMOUNT\", \"PASSENGER_COUNT\", \"PULOCATIONID\", \"DOLOCATIONID\", \"PICKUP_TS\", \"DROPOFF_TS\", vec_window_end(\"DROPOFF_TS\", '15m') AS \"WINDOW_END\" FROM ( SELECT  *  FROM ( SELECT \"TRIP_DISTANCE\", \"FARE_AMOUNT\", \"PASSENGER_COUNT\", \"PULOCATIONID\", \"DOLOCATIONID\",  CAST ((\"TPEP_PICKUP_DATETIME\" / 1000000 :: INT) AS TIMESTAMP) AS \"PICKUP_TS\",  CAST ((\"TPEP_DROPOFF_DATETIME\" / 1000000 :: INT) AS TIMESTAMP) AS \"DROPOFF_TS\" FROM SNOWML_FEATURE_STORE_TEST_DB.TEST_DATASET.yellow_tripdata_2016_01) WHERE DROPOFF_TS >= '2016-01-01 00:00:00' AND DROPOFF_TS < '2016-01-03 00:00:00')) GROUP BY \"DOLOCATIONID\", \"WINDOW_END\"), _version=V1, _status=FeatureViewStatus.RUNNING, _feature_desc=OrderedDict([('COUNT_TRIP_2_HR', None), ('COUNT_TRIP_5_HR', None)]), _refresh_freq=1 minute, _database=FS_TIME_SERIES_EXAMPLE, _schema=AWESOME_FS, _warehouse=PUBLIC)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropoff_fv = FeatureView(name=\"trip_dropoff_time_series_features\", entities=[trip_dropoff], feature_df=dropoff_df, timestamp_col=\"ts\")\n",
    "fs.register_feature_view(feature_view=dropoff_fv, version=\"v1\", refresh_freq=\"1 minute\", block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009c81",
   "metadata": {},
   "source": [
    "## Explore FeatureViews\n",
    "We can easily discover what are the materialized FeatureViews and the corresponding features with *__fs.list_feature_views()__*. \n",
    "\n",
    "We can also apply filters based on Entity name or FeatureView names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bc93de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                            |\"VERSION\"  |\"ENTITIES\"                                          |\"FEATURE_DESC\"             |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|TRIP_PICKUP_TIME_SERIES_FEATURES  |V1         |[                                                   |{                          |\n",
      "|                                  |           |  \"{\\\"name\\\": \\\"TRIP_PICKUP\\\", \\\"join_keys\\\": [...  |  \"MEAN_FARE_2_HR\": null,  |\n",
      "|                                  |           |]                                                   |  \"MEAN_FARE_5_HR\": null   |\n",
      "|                                  |           |                                                    |}                          |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.list_feature_views(entity_name=\"trip_pickup\").select([\"NAME\", \"VERSION\", \"ENTITIES\", \"FEATURE_DESC\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302cf23",
   "metadata": {},
   "source": [
    "## Generate training data and train a model\n",
    "The training data generation will lookup __point-in-time correct__ feature values and join with the spine dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4e3376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PULOCATIONID\"  |\"DOLOCATIONID\"  |\"PICKUP_TS\"          |\"FARE_AMOUNT\"  |\"MEAN_FARE_2_HR\"    |\"MEAN_FARE_5_HR\"    |\"COUNT_TRIP_2_HR\"  |\"COUNT_TRIP_5_HR\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|132             |219             |2016-01-01 00:08:08  |20.0           |NULL                |NULL                |NULL               |NULL               |\n",
      "|132             |219             |2016-01-01 00:28:28  |14.0           |37.342592592592595  |37.342592592592595  |NULL               |NULL               |\n",
      "|164             |219             |2016-01-01 00:29:09  |55.0           |12.699115044247788  |12.699115044247788  |NULL               |NULL               |\n",
      "|163             |219             |2016-01-01 00:31:08  |52.0           |12.61353711790393   |12.61353711790393   |1                  |1                  |\n",
      "|132             |219             |2016-01-01 00:31:38  |18.5           |37.40517857142857   |37.40517857142857   |1                  |1                  |\n",
      "|132             |219             |2016-01-01 00:35:37  |13.5           |37.40517857142857   |37.40517857142857   |1                  |1                  |\n",
      "|132             |219             |2016-01-01 00:40:44  |12.5           |37.40517857142857   |37.40517857142857   |1                  |1                  |\n",
      "|114             |219             |2016-01-01 00:59:52  |52.0           |12.618020304568528  |12.618020304568528  |4                  |4                  |\n",
      "|170             |219             |2016-01-01 01:08:24  |46.5           |11.720843672456576  |11.720843672456576  |6                  |6                  |\n",
      "|41              |219             |2016-01-01 01:45:59  |55.5           |12.363905325443787  |12.363905325443787  |9                  |9                  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT * FROM FS_TIME_SERIES_EXAMPLE.AWESOME_FS.yellow_tripdata_2016_01_training_data_2023_09_20_13_31_23'],\n",
       " 'post_actions': []}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spine_df = source_df.select([\"PULOCATIONID\", \"DOLOCATIONID\", \"PICKUP_TS\", \"FARE_AMOUNT\"])\n",
    "training_data = fs.generate_dataset(\n",
    "    spine_df=spine_df,\n",
    "    features=[pickup_fv, dropoff_fv],\n",
    "    materialized_table=\"yellow_tripdata_2016_01_training_data\",\n",
    "    spine_timestamp_col=\"PICKUP_TS\",\n",
    "    spine_label_cols = [\"FARE_AMOUNT\"]\n",
    ")\n",
    "\n",
    "training_data.df.show()\n",
    "training_data.df.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6bced5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULOCATIONID</th>\n",
       "      <th>DOLOCATIONID</th>\n",
       "      <th>MEAN_FARE_2_HR</th>\n",
       "      <th>MEAN_FARE_5_HR</th>\n",
       "      <th>COUNT_TRIP_2_HR</th>\n",
       "      <th>COUNT_TRIP_5_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72133</th>\n",
       "      <td>211</td>\n",
       "      <td>68</td>\n",
       "      <td>9.953704</td>\n",
       "      <td>10.302292</td>\n",
       "      <td>564.0</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549762</th>\n",
       "      <td>79</td>\n",
       "      <td>225</td>\n",
       "      <td>10.022312</td>\n",
       "      <td>10.379619</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129708</th>\n",
       "      <td>261</td>\n",
       "      <td>211</td>\n",
       "      <td>14.340517</td>\n",
       "      <td>14.800613</td>\n",
       "      <td>200.0</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108349</th>\n",
       "      <td>100</td>\n",
       "      <td>161</td>\n",
       "      <td>14.896985</td>\n",
       "      <td>14.896985</td>\n",
       "      <td>305.0</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474535</th>\n",
       "      <td>236</td>\n",
       "      <td>262</td>\n",
       "      <td>10.602582</td>\n",
       "      <td>10.602582</td>\n",
       "      <td>563.0</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PULOCATIONID  DOLOCATIONID  MEAN_FARE_2_HR  MEAN_FARE_5_HR  \\\n",
       "72133            211            68        9.953704       10.302292   \n",
       "549762            79           225       10.022312       10.379619   \n",
       "129708           261           211       14.340517       14.800613   \n",
       "108349           100           161       14.896985       14.896985   \n",
       "474535           236           262       10.602582       10.602582   \n",
       "\n",
       "        COUNT_TRIP_2_HR  COUNT_TRIP_5_HR  \n",
       "72133             564.0           1008.0  \n",
       "549762             13.0             38.0  \n",
       "129708            200.0            259.0  \n",
       "108349            305.0            305.0  \n",
       "474535            563.0            563.0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "training_pd = training_data.df.to_pandas()\n",
    "X = training_pd.drop([\"FARE_AMOUNT\", \"PICKUP_TS\"], axis=1)\n",
    "y = training_pd[\"FARE_AMOUNT\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f0e6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.74351768372038 %\n",
      "Mean squared error: 90.17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "estimator = make_pipeline(imp, LinearRegression())\n",
    "\n",
    "reg = estimator.fit(X, y)\n",
    "r2_score = reg.score(X_test, y_test)\n",
    "print(r2_score * 100,'%')\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142c25c",
   "metadata": {},
   "source": [
    "## Log model with Model Registry\n",
    "We can log the model along with its training dataset metadata with model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c57a81e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The database \"my_cool_registry\" already exists. Skipping creation.\n",
      "WARNING:absl:The schema \"my_cool_registry\"._SYSTEM_MODEL_REGISTRY_SCHEMA already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.registry import model_registry\n",
    "import time\n",
    "\n",
    "registry = model_registry.ModelRegistry(session=session, database_name=\"my_cool_registry\", create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a935926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/ml/model/model_signature.py:52: UserWarning: The sample input has 653271 rows, thus a truncation happened before inferring signature. This might cause inaccurate signature inference. If that happens, consider specifying signature manually.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"my_model_{time.time()}\"\n",
    "\n",
    "model_ref = registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=\"v1\",\n",
    "    model=estimator,\n",
    "    dataset=training_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0581d",
   "metadata": {},
   "source": [
    "## Restore model and predict with latest features\n",
    "We retrieve the training dataset from registry then construct dataframe of latest feature values. Then we restore the model from registry. At last, we can predict with latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "999a633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/session.py:1833: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.numeric.Int64Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, nchunks, nrows, ci_output = write_pandas(\n"
     ]
    }
   ],
   "source": [
    "# Prepare some source prediction data\n",
    "pred_df = training_pd.sample(3, random_state=996)[['PULOCATIONID', 'DOLOCATIONID', 'PICKUP_TS']]\n",
    "pred_df = session.create_dataframe(pred_df)\n",
    "pred_df = pred_df.select('PULOCATIONID', 'DOLOCATIONID', F.cast(pred_df.PICKUP_TS / 1000000, TimestampType()).alias('PICKUP_TS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0a18a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich source prediction data with features\n",
    "registered_training_data = registry.get_dataset(\n",
    "    model_name=\"my_trained_model\", \n",
    "    model_version=\"v1\",\n",
    ")\n",
    "\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    spine_df=pred_df, \n",
    "    features=registered_training_data.load_features(), \n",
    "    spine_timestamp_col='PICKUP_TS'\n",
    ").drop(['PICKUP_TS']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3bd545ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.75684447  8.77725855 12.42049179]\n"
     ]
    }
   ],
   "source": [
    "model_ref = model_registry.ModelReference(\n",
    "    registry=registry, \n",
    "    model_name=model_name, \n",
    "    model_version=\"v1\"\n",
    ").load_model()\n",
    "\n",
    "pred = model_ref.predict(enriched_df)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad5af0",
   "metadata": {},
   "source": [
    "## DO NOT READ\n",
    "Below is a simple test for the window_end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d45ba589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark.session:The version of package 'numpy' in the local environment is 1.24.4, which does not fit the criteria for the requirement 'numpy'. Your UDF might not work when the package version is different between the server and your local environment.\n",
      "WARNING:snowflake.snowpark.session:Package 'pytimeparse' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "ERROR:snowflake.snowpark._internal.server_connection:Failed to execute query [queryID: 01af19db-0406-b1b7-000c-a90273b47663] \n",
      "CREATE OR REPLACE \n",
      "TEMPORARY  FUNCTION  window_end(arg1 TIMESTAMP,arg2 STRING)\n",
      "RETURNS TIMESTAMP\n",
      "LANGUAGE PYTHON \n",
      "RUNTIME_VERSION=3.8\n",
      "\n",
      "PACKAGES=('numpy','pandas','pytimeparse','cloudpickle==2.0.0')\n",
      "\n",
      "\n",
      "HANDLER='compute'\n",
      "\n",
      "AS $$\n",
      "import pickle\n",
      "\n",
      "func = pickle.loads(bytes.fromhex('8005955b030000000000008c17636c6f75647069636b6c652e636c6f75647069636b6c65948c0d5f6275696c74696e5f747970659493948c0a4c616d6264615479706594859452942868028c08436f6465547970659485945294284b024b004b004b074b054b434376640164006c007d02640164006c017d03640164026c026d037d0401007c047c016401190083017d057c0564006b087242740464037c01640119009b009d02830182017c007c02a0056404a10118007c02a00664056406a1021a007c051a007c0514007c0517007d067c036a077c06640664078d02530094284e4b008c0974696d6570617273659485948c1643616e6e6f7420706172736520696e74657276616c20948c13313937302d30312d30315430303a30303a3030944b018c0173948c04756e69749485947494288c056e756d7079948c0670616e646173948c15707974696d6570617273652e74696d65706172736594680a8c0a56616c75654572726f72948c0a6461746574696d653634948c0b74696d6564656c74613634948c0b746f5f6461746574696d65947494288c0178948c08696e74657276616c948c026e70948c02706494680a8c0a74696d655f736c696365948c0974696d655f736c6f749474948c4e2f7661722f666f6c646572732f67792f7738777931727931356c6a31326e6e37776c7130356a743030303030676e2f542f6970796b65726e656c5f39313232332f323334313939343137392e7079948c167665635f77696e646f775f656e645f636f6d70757465944b0a4310000a080108010c020c01080112012601942929749452947d94288c0b5f5f7061636b6167655f5f944e8c085f5f6e616d655f5f948c085f5f6d61696e5f5f94754e4e4e749452948c1c636c6f75647069636b6c652e636c6f75647069636b6c655f66617374948c125f66756e6374696f6e5f7365747374617465949394682b7d947d9428682868228c0c5f5f7175616c6e616d655f5f9468228c0f5f5f616e6e6f746174696f6e735f5f947d948c0e5f5f6b7764656661756c74735f5f944e8c0c5f5f64656661756c74735f5f944e8c0a5f5f6d6f64756c655f5f9468298c075f5f646f635f5f944e8c0b5f5f636c6f737572655f5f944e8c175f636c6f75647069636b6c655f7375626d6f64756c6573945d948c0b5f5f676c6f62616c735f5f947d947586948652302e'))\n",
      "# The following comment contains the source code generated by snowpark-python for explanatory purposes.\n",
      "# @F.pandas_udf(\n",
      "#     name=udf_name,\n",
      "#     replace=True,\n",
      "#     packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
      "#     session=session,\n",
      "# )\n",
      "# def vec_window_end_compute(\n",
      "#     x: T.PandasSeries[datetime.datetime],\n",
      "#     interval: T.PandasSeries[str],\n",
      "# ) -> T.PandasSeries[datetime.datetime]:\n",
      "#     import numpy as np\n",
      "#     import pandas as pd\n",
      "#     from pytimeparse.timeparse import timeparse\n",
      "#\n",
      "#     time_slice = timeparse(interval[0])\n",
      "#     if time_slice is None:\n",
      "#         raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
      "#     time_slot = (x - np.datetime64('1970-01-01T00:00:00')) // np.timedelta64(1, 's') // time_slice * time_slice + time_slice\n",
      "#     return pd.to_datetime(time_slot, unit='s')\n",
      "#\n",
      "# func = vec_window_end_compute\n",
      "\n",
      "\n",
      "\n",
      "from threading import RLock\n",
      "\n",
      "lock = RLock()\n",
      "\n",
      "class InvokedFlag:\n",
      "    def __init__(self):\n",
      "        self.invoked = False\n",
      "\n",
      "def lock_function_once(f, flag):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        if not flag.invoked:\n",
      "            with lock:\n",
      "                if not flag.invoked:\n",
      "                    result = f(*args, **kwargs)\n",
      "                    flag.invoked = True\n",
      "                    return result\n",
      "                return f(*args, **kwargs)\n",
      "        return f(*args, **kwargs)\n",
      "    return wrapper\n",
      "\n",
      "\n",
      "invoked = InvokedFlag()\n",
      "\n",
      "def compute(df):\n",
      "    return lock_function_once(func, invoked)(*[df[idx] for idx in range(df.shape[1])])\n",
      "\n",
      "import pandas\n",
      "\n",
      "compute._sf_vectorized_input = pandas.DataFrame\n",
      "$$\n",
      "\n",
      "\n",
      "002002 (42710): SQL compilation error:\n",
      "Object 'WINDOW_END(ARG1 TIMESTAMP_NTZ, ARG2 VARCHAR):TIMESTAMP_NTZ(9)' already exists.\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01af19db-0406-b1b7-000c-a90273b47663: 002002 (42710): SQL compilation error:\nObject 'WINDOW_END(ARG1 TIMESTAMP_NTZ, ARG2 VARCHAR):TIMESTAMP_NTZ(9)' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m session \u001b[38;5;241m=\u001b[39m Session\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfigs(SnowflakeLoginOptions())\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m      8\u001b[0m udf_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129;43m@F\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mudf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytimeparse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mvec_window_end_compute\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPandasSeries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPandasSeries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPandasSeries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mnumpy\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mnp\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mpandas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mpd\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/udf.py:601\u001b[0m, in \u001b[0;36mUDFRegistration.register\u001b[0;34m(self, func, return_type, input_types, name, is_permanent, stage_location, imports, packages, replace, if_not_exists, parallel, max_batch_size, strict, secure, external_access_integrations, secrets, statement_params, source_code_display, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m _from_pandas \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pandas_udf_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# register udf\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_register_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstage_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_not_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_from_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecrets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_code_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_code_display\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_call_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUDFRegistration.register\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[pandas_udf]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_from_pandas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_permanent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/udf.py:881\u001b[0m, in \u001b[0;36mUDFRegistration._do_register_udf\u001b[0;34m(self, func, return_type, input_types, name, stage_location, imports, packages, replace, if_not_exists, parallel, max_batch_size, from_pandas_udf_function, strict, secure, external_access_integrations, secrets, statement_params, source_code_display, api_call_source, skip_upload_on_content_match, is_permanent)\u001b[0m\n\u001b[1;32m    877\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    878\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    879\u001b[0m         pe\n\u001b[1;32m    880\u001b[0m     )\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m     raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/udf.py:852\u001b[0m, in \u001b[0;36mUDFRegistration._do_register_udf\u001b[0;34m(self, func, return_type, input_types, name, stage_location, imports, packages, replace, if_not_exists, parallel, max_batch_size, from_pandas_udf_function, strict, secure, external_access_integrations, secrets, statement_params, source_code_display, api_call_source, skip_upload_on_content_match, is_permanent)\u001b[0m\n\u001b[1;32m    850\u001b[0m raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 852\u001b[0m     \u001b[43mcreate_python_udf_or_sp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTempObjectType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFUNCTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mudf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_imports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_packages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_packages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_permanent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_not_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_not_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43minline_python_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_call_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_call_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecrets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# an exception might happen during registering a udf\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;66;03m# (e.g., a dependency might not be found on the stage),\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# then for a permanent udf, we should delete the uploaded\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# python file and raise the exception\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProgrammingError \u001b[38;5;28;01mas\u001b[39;00m pe:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/_internal/udf_utils.py:1031\u001b[0m, in \u001b[0;36mcreate_python_udf_or_sp\u001b[0;34m(session, return_type, input_args, handler, object_type, object_name, all_imports, all_packages, is_permanent, replace, if_not_exists, inline_python_code, execute_as, api_call_source, strict, secure, external_access_integrations, secrets)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     secrets_in_sql \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSECRETS=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39msecrets\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m secrets\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m     create_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;124mCREATE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR REPLACE \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mreplace\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mis_permanent\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPORARY\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSECURE\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msecure\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIF NOT EXISTS\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mif_not_exists\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql_func_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;132;01m{\u001b[39;00minline_python_code_in_sql\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m-> 1031\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# fire telemetry after _run_query is successful\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     api_call_source \u001b[38;5;241m=\u001b[39m api_call_source \u001b[38;5;129;01more\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_internal.create_python_udf_or_sp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/session.py:1690\u001b[0m, in \u001b[0;36mSession._run_query\u001b[0;34m(self, query, is_ddl_on_temp_object, log_on_exception)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_query\u001b[39m(\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1686\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1687\u001b[0m     is_ddl_on_temp_object: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m     log_on_exception: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1689\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m-> 1690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:102\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m     99\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:96\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m     99\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:366\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:347\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_statement_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 347\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    349\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    351\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/connector/cursor.py:908\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[1;32m    904\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    905\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m    907\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m--> 908\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/connector/errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    298\u001b[0m             error_class,\n\u001b[1;32m    299\u001b[0m             error_value,\n\u001b[1;32m    300\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/connector/errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fs_demo/lib/python3.8/site-packages/snowflake/connector/errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    232\u001b[0m )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01af19db-0406-b1b7-000c-a90273b47663: 002002 (42710): SQL compilation error:\nObject 'WINDOW_END(ARG1 TIMESTAMP_NTZ, ARG2 VARCHAR):TIMESTAMP_NTZ(9)' already exists."
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "import datetime\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()\n",
    "\n",
    "udf_name = \"window_end\"\n",
    "    \n",
    "@F.pandas_udf(\n",
    "    name=udf_name,\n",
    "    replace=True,\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) // np.timedelta64(1, 's') // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')\n",
    "\n",
    "df = session.create_dataframe(\n",
    "    [\n",
    "        '2023-01-31 01:02:03.004',\n",
    "        '2023-01-31 01:14:59.999',\n",
    "        '2023-01-31 01:15:00.000',\n",
    "        '2023-01-31 01:15:00.004',\n",
    "        '2023-01-31 01:17:10.007',\n",
    "    ], \n",
    "    schema=['a']\n",
    ")\n",
    "df = df.select([F.to_timestamp(\"a\").alias(\"ts\")])\n",
    "\n",
    "df = df.select([\"TS\", F.call_udf(udf_name, F.col(\"TS\"), \"15m\").alias(\"window_end\")])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67a5a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(WINDOW_END(TS, '15M')=datetime.datetime(2023, 1, 31, 1, 15)),\n",
       " Row(WINDOW_END(TS, '15M')=datetime.datetime(2023, 1, 31, 1, 15)),\n",
       " Row(WINDOW_END(TS, '15M')=datetime.datetime(2023, 1, 31, 1, 30)),\n",
       " Row(WINDOW_END(TS, '15M')=datetime.datetime(2023, 1, 31, 1, 30)),\n",
       " Row(WINDOW_END(TS, '15M')=datetime.datetime(2023, 1, 31, 1, 30))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"select window_end(ts, '15m') from foobar\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
