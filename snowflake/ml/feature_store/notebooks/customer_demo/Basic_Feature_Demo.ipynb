{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb54abc",
   "metadata": {},
   "source": [
    "- Required snowflake-ml-python version **1.5.0** or higher\n",
    "- Required snowflake version **8.17** or higher\n",
    "- Updated on: 5/5/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae3429",
   "metadata": {},
   "source": [
    "## Basic Feature Demo\n",
    "\n",
    "This notebook demonstrates feature store with simple features. It includes an end-2-end ML experiment cycle: feature creation, training and inference. It also demonstrate the interoperation between Feature Store and Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714787e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16e6a8",
   "metadata": {},
   "source": [
    "## Setup Snowflake connection and database\n",
    "For detailed session connection config, please follow this [tutorial](https://medium.com/snowflake/snowflakeloginoptions-an-easier-way-to-connect-using-python-2f0e726da936).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e1503",
   "metadata": {},
   "source": [
    "Below cell creates temporary database, schema and warehouse for this notebook. All temporary resources will be cleaned up at the end of this notebook. You can rename with your own name if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9622928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# database name where test data, feature store and model lives.\n",
    "FS_DEMO_DB = f\"FEATURE_STORE_BASIC_FEATURE_NOTEBOOK_DEMO\"\n",
    "# schema where test data lives.\n",
    "TEST_DATASET_SCHEMA = 'TEST_DATASET'\n",
    "# feature store name.\n",
    "FS_DEMO_SCHEMA = \"AWESOME_FS_BASIC_FEATURES\"\n",
    "# the schema model lives.\n",
    "MODEL_DEMO_SCHEMA = \"MODELS\"\n",
    "# warehouse name used in this notebook.\n",
    "FS_DEMO_WH = \"FEATURE_STORE_BASIC_FEATURE_NOTEBOOK_DEMO\"\n",
    "\n",
    "session.sql(f\"CREATE OR REPLACE DATABASE {FS_DEMO_DB}\").collect()\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE SCHEMA {FS_DEMO_DB}.{TEST_DATASET_SCHEMA}\n",
    "\"\"\").collect()\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE SCHEMA {FS_DEMO_DB}.{MODEL_DEMO_SCHEMA}\n",
    "\"\"\").collect()\n",
    "session.sql(f\"CREATE WAREHOUSE IF NOT EXISTS {FS_DEMO_WH}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece7a2b",
   "metadata": {},
   "source": [
    "## Create a new FeatureStore client\n",
    "\n",
    "Let's first create a feature store client. With `CREATE_IF_NOT_EXIST` mode, it will try to create schema and all necessary feature store metadata if it doesn't exist already. It is required for the first time to setup a Feature Store. Afterwards, you can use `FAIL_IF_NOT_EXIST` mode to connecte to an existing Feature Store. \n",
    "\n",
    "Note database must already exist. Feature Store will **NOT** try to create the database even in `CREATE_IF_NOT_EXIST` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe850ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=FS_DEMO_DB, \n",
    "    name=FS_DEMO_SCHEMA, \n",
    "    default_warehouse=FS_DEMO_WH,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ba9be",
   "metadata": {},
   "source": [
    "## Prepare test data\n",
    "\n",
    "We will use wine quality dataset for this demo. Download the public dataset from kaggle if you dont have it already: https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009. Replace `TEST_CSV_FILE_PATH` with your local file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60407c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_FILE_PATH = 'winequality-red.csv'\n",
    "session.file.put(\n",
    "    f\"file://{TEST_CSV_FILE_PATH}\",session.get_session_stage())\n",
    "\n",
    "from snowflake.snowpark.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    IntegerType, \n",
    "    FloatType\n",
    ")\n",
    "input_schema = StructType(\n",
    "    [\n",
    "        StructField(\"fixed_acidity\", FloatType()), \n",
    "        StructField(\"volatile_acidity\", FloatType()), \n",
    "        StructField(\"citric_acid\", FloatType()), \n",
    "        StructField(\"residual_sugar\", FloatType()), \n",
    "        StructField(\"chlorides\", FloatType()), \n",
    "        StructField(\"free_sulfur_dioxide\", IntegerType()),\n",
    "        StructField(\"total_sulfur_dioxide\", IntegerType()), \n",
    "        StructField(\"density\", FloatType()), \n",
    "        StructField(\"pH\", FloatType()), \n",
    "        StructField(\"sulphates\", FloatType()),\n",
    "        StructField(\"alcohol\", FloatType()), \n",
    "        StructField(\"quality\", IntegerType())\n",
    "    ]\n",
    ")\n",
    "df = session.read.options({\"field_delimiter\": \";\", \"skip_header\": 1}) \\\n",
    "    .schema(input_schema) \\\n",
    "    .csv(f\"{session.get_session_stage()}/winequality-red.csv\")\n",
    "full_table_name = f\"{FS_DEMO_DB}.{TEST_DATASET_SCHEMA}.WINE_DATA\"\n",
    "df.write.mode(\"overwrite\").save_as_table(full_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b7ad1",
   "metadata": {},
   "source": [
    "## Create and register a new Entity\n",
    "\n",
    "We will create an Entity called *wine* and register it with the feature store.\n",
    "\n",
    "You can retrieve the active Entities in the feature store with list_entities() API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_entity = Entity(name=\"WINE\", join_keys=[\"WINE_ID\"])\n",
    "fs.register_entity(wine_entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a529d7",
   "metadata": {},
   "source": [
    "## Load source data and do some simple feature engineering\n",
    "\n",
    "Then we will load from the source table and conduct some simple feature engineerings.\n",
    "\n",
    "Here we are just doing two simple data manipulation (but more complex ones are carried out the same way):\n",
    "1. Assign a WINE_ID column to the source\n",
    "2. Derive a new column by multipying two existing feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6037ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "def addIdColumn(df, id_column_name):\n",
    "    # Add id column to dataframe\n",
    "    columns = df.columns\n",
    "    new_df = df.withColumn(\n",
    "        id_column_name,\n",
    "        F.row_number().over(Window.order_by(F.col(\"quality\"))))\n",
    "    return new_df\n",
    "\n",
    "source_df = session.table(full_table_name)\n",
    "source_df = addIdColumn(source_df, \"WINE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e7611-d900-443f-85b0-6425640e0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df_rows_count = source_df.count()\n",
    "print(f\"Total number of rows in source df: {source_df_rows_count}\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_feature(df):\n",
    "    # Derive a new feature column\n",
    "    new_df = df.withColumn(\n",
    "        \"MY_NEW_FEATURE\", df[\"FIXED_ACIDITY\"] * df[\"CITRIC_ACID\"])\n",
    "    return new_df.select([\n",
    "        'WINE_ID',\n",
    "        'FIXED_ACIDITY',\n",
    "        'VOLATILE_ACIDITY',\n",
    "        'CITRIC_ACID',\n",
    "        'RESIDUAL_SUGAR',\n",
    "        'CHLORIDES',\n",
    "        'FREE_SULFUR_DIOXIDE',\n",
    "        'TOTAL_SULFUR_DIOXIDE',\n",
    "        'DENSITY',\n",
    "        'PH',\n",
    "        'MY_NEW_FEATURE',\n",
    "    ])\n",
    "\n",
    "feature_df = generate_new_feature(source_df)\n",
    "feature_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4be7da",
   "metadata": {},
   "source": [
    "## Create a new FeatureView and materialize the feature pipeline\n",
    "\n",
    "Now we construct a Feature View with above DataFrame. We firstly create a draft feature view. We set the `refresh_freq` to 1 minute, so it will be refreshed every 1 minute. On the backend, it creates a Snowflake [dynamic table](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro). At this point, the draft feature view will not take effect because it is not registered yet.\n",
    "Then we register the feature view by via `register_feature_view`. It will materialize to Snowflake backend. [Incremental maintenance](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh#label-dynamic-tables-intro-refresh-queries) will start if the query is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_fv = FeatureView(\n",
    "    name=\"WINE_FEATURES\", \n",
    "    entities=[wine_entity], \n",
    "    feature_df=feature_df, \n",
    "    refresh_freq=\"1 minute\", \n",
    "    desc=\"my wine features auto refreshed on a schedule\"\n",
    ")\n",
    "wine_features = fs.register_feature_view(\n",
    "    feature_view=draft_fv, \n",
    "    version=\"1.0\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a920a5-4a88-48af-8540-e695dd6c60cd",
   "metadata": {},
   "source": [
    "We can examine the feature values in a feature view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.read_feature_view(wine_features).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fc02a",
   "metadata": {},
   "source": [
    "## Explore additional features\n",
    "\n",
    "Now I have my FeatureView created with a collection of features, but what if I want to explore additional features on top?\n",
    "\n",
    "Since a materialized FeatureView is immutable, we can create a new FeatureView for the additional features. Note `refresh_freq` of below Feature View is None. It means the Feature View is static and will not refresh on a schedule. You can still update the feature values by updating the data source (table `WINE_DATA`). On the backend it is a Snowflake view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_feature_df = source_df.select([\n",
    "    'WINE_ID',\n",
    "    'SULPHATES',\n",
    "    'ALCOHOL',\n",
    "])\n",
    "\n",
    "extra_draft_fv = FeatureView(\n",
    "    name=\"EXTRA_WINE_FEATURES\", \n",
    "    entities=[wine_entity], \n",
    "    feature_df=extra_feature_df, \n",
    "    refresh_freq=None, \n",
    "    desc=\"extra wine features\"\n",
    ")\n",
    "extra_features = fs.register_feature_view(\n",
    "    feature_view=extra_draft_fv, \n",
    "    version=\"1.0\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096136c-f6c5-4131-830d-1fa2aa7431d6",
   "metadata": {},
   "source": [
    "We can examine the status of all feature views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.list_feature_views(entity_name=\"WINE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1a7dc",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n",
    "After our feature pipelines are fully setup, we can start using them to generate training data and later do model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_df = source_df.select(\"WINE_ID\", \"QUALITY\")\n",
    "spine_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad126af",
   "metadata": {},
   "source": [
    "Generate training data is easy since materialized FeatureViews already carry most of the metadata like join keys, timestamp for point-in-time lookup, etc. We just need to provide the spine data (it's called spine because we are essentially enriching the data by joining features with it). We can also generate dataset with a subset of features in the feature view by `slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = fs.generate_dataset(\n",
    "    name=\"my_training_dataset\",\n",
    "    version=\"12\",\n",
    "    spine_df=spine_df, \n",
    "    features=[\n",
    "        wine_features.slice([\n",
    "            \"FIXED_ACIDITY\", \"VOLATILE_ACIDITY\", \"CITRIC_ACID\"]), \n",
    "        extra_features\n",
    "    ],\n",
    "    spine_timestamp_col=None, \n",
    "    spine_label_cols=[\"QUALITY\"],\n",
    "    exclude_columns=['WINE_ID'],\n",
    "    desc=\"my training dataset with EXTRA_WINE_FEATURES and WINE_FEATURES\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cf2f4-59b3-40da-8c43-bee27129105d",
   "metadata": {},
   "source": [
    "Convert dataset to a snowpark dataframe and examine all the features in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c71aa-1c6b-4bf4-83f9-2176dc249f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df = my_dataset.read.to_snowpark_dataframe()\n",
    "assert training_data_df.count() == source_df_rows_count\n",
    "training_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca7543",
   "metadata": {},
   "source": [
    "## Train model with Snowpark ML\n",
    "\n",
    "Now let's training a simple random forest model, and evaluate the prediction accuracy. When you call `fit()` on a DataFrame that converted from Feature Store Dataset, The linkage between model and dataset is automatically wired up. Later, you can easily retrieve the dataset from this model, or you can query the lineage about the dataset and model. This is work-in-progress and will be ready soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352603a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.ensemble import RandomForestRegressor\n",
    "from snowflake.ml.modeling import metrics as snowml_metrics\n",
    "from snowflake.snowpark.functions import abs as sp_abs, mean, col\n",
    "\n",
    "def train_model_using_snowpark_ml(training_data_df):\n",
    "    train, test = training_data_df.random_split([0.8, 0.2], seed=42)\n",
    "    feature_columns = \\\n",
    "        [col for col in training_data_df.columns if col != \"QUALITY\"]\n",
    "    label_column = \"QUALITY\"\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        input_cols=feature_columns, label_cols=[label_column], \n",
    "        max_depth=3, n_estimators=20, random_state=42\n",
    "    )\n",
    "\n",
    "    rf.fit(train)\n",
    "    predictions = rf.predict(test)\n",
    "\n",
    "    mse = snowml_metrics.mean_squared_error(\n",
    "        df=predictions, \n",
    "        y_true_col_names=label_column, \n",
    "        y_pred_col_names=\"OUTPUT_\" + label_column)\n",
    "\n",
    "    accuracy = 100 - snowml_metrics.mean_absolute_percentage_error(\n",
    "        df=predictions,\n",
    "        y_true_col_names=label_column,\n",
    "        y_pred_col_names=\"OUTPUT_\" + label_column\n",
    "    )\n",
    "\n",
    "    print(f\"MSE: {mse}, Accuracy: {accuracy}\")\n",
    "    return rf\n",
    "\n",
    "rf = train_model_using_snowpark_ml(training_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8031f",
   "metadata": {},
   "source": [
    "## [Predict Optional 1] With local model\n",
    "Now we can predict with a local model and the feature values retrieved from feature store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spine_df.limit(3).select(\"WINE_ID\")\n",
    "\n",
    "# load back feature views from dataset\n",
    "fvs = fs.load_feature_views_from_dataset(my_dataset)\n",
    "enriched_df = fs.retrieve_feature_values(test_df, fvs)\n",
    "enriched_df = enriched_df.drop('WINE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(enriched_df.to_pandas())\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b81639",
   "metadata": {},
   "source": [
    "## [Predict Option 2] With Model Registry\n",
    "We can also predict with models in [Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry).\n",
    "\n",
    "### Step 1 : Log the model into Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f232fe-126f-433d-8b19-953afee42632",
   "metadata": {},
   "source": [
    "Firstly, we connect to a model registry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a29768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "registry = Registry(\n",
    "    session=session, \n",
    "    database_name=FS_DEMO_DB, \n",
    "    schema_name=MODEL_DEMO_SCHEMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd12070",
   "metadata": {},
   "source": [
    "Then we log the model to model registry. Later, we can get it back with same model name and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MY_RANDOM_FOREST_REGRESSOR_MODEL\"\n",
    "\n",
    "registry.log_model(\n",
    "    model_name=model_name,\n",
    "    version_name=\"V2\",\n",
    "    model=rf,\n",
    "    comment=\"log my model trained with dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf2743",
   "metadata": {},
   "source": [
    "### Step 2 : Restore model and predict with features\n",
    "\n",
    "We read the model back from model registry. We get the features from the dataset, and retrieve latest values for these features from Feature Store. We will same features that the model previously trained on for future inference.\n",
    "\n",
    "We are working on retrieving dataset from a model directly. For now, we just use previously created dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = registry.get_model(model_name).version(\"V2\")\n",
    "\n",
    "# We are working on loading dataset back from a model. \n",
    "# For now, we use previously created dataset. \n",
    "fvs = fs.load_feature_views_from_dataset(my_dataset)\n",
    "spine_df = spine_df.limit(3).select(\"WINE_ID\")\n",
    "\n",
    "enriched_df =fs.retrieve_feature_values(\n",
    "    spine_df=spine_df, \n",
    "    features=fvs, \n",
    "    exclude_columns=[\"WINE_ID\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a413f50-72d7-477d-8c51-3b25ddf4e7d4",
   "metadata": {},
   "source": [
    "Now we predict on the model and latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_prediction = model.run(\n",
    "    enriched_df.to_pandas(), function_name=\"predict\")\n",
    "\n",
    "print(restored_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173da73",
   "metadata": {},
   "source": [
    "## Cleanup notebook\n",
    "Cleanup resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"DROP DATABASE IF EXISTS {FS_DEMO_DB}\").collect()\n",
    "session.sql(f\"DROP WAREHOUSE IF EXISTS {FS_DEMO_WH}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
