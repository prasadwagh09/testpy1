{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f029c96",
   "metadata": {},
   "source": [
    "Notebook version: 0.1.0\n",
    "\n",
    "Updated date: 10/10/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160eb3e",
   "metadata": {},
   "source": [
    "## Time Series Features Demo\n",
    "This notebook demonstrates feature store with time series features. It includes an end-2-end ML experiment cycle: feature creation, training and inference. It also demonstrate the interoperation between Feature Store and Model Registry.\n",
    "\n",
    "It uses public NY taxi trip data to compute features. The public data can be downloaded from: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark.types import TimestampType\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09ef0d",
   "metadata": {},
   "source": [
    "## Setup Snowflake connection\n",
    "For detailed session connection config, please follow this [tutorial](https://medium.com/snowflake/snowflakeloginoptions-an-easier-way-to-connect-using-python-2f0e726da936)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25723426",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67136a",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e75f08",
   "metadata": {},
   "source": [
    "Download Yellow Taxi Trip Records data (Jan. 2016) from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page if you don't have it already. Rename `PARQUET_FILE_LOCAL_PATH` with your local file path. Rename `TEST_DATASET_DB` and `TEST_DATASET_SCHEMA` with your location.\n",
    "\n",
    "Below code create a table with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE_NAME = f\"yellow_tripdata_2016-01.parquet\"\n",
    "PARQUET_FILE_LOCAL_PATH = f\"file://~/Downloads/{PARQUET_FILE_NAME}\"\n",
    "TEST_DATASET_DB = \"SNOWML_FEATURE_STORE_TEST_DB\"\n",
    "TEST_DATASET_SCHEMA = 'TEST_DATASET'\n",
    "\n",
    "def get_destination_table_name(original_file_name: str) -> str:\n",
    "    return original_file_name.split(\".\")[0].replace(\"-\", \"_\").upper()\n",
    "\n",
    "table_name = get_destination_table_name(PARQUET_FILE_NAME)\n",
    "session.file.put(PARQUET_FILE_LOCAL_PATH, session.get_session_stage())\n",
    "\n",
    "df = session.read \\\n",
    "    .parquet(f\"{session.get_session_stage()}/{PARQUET_FILE_NAME}\")\n",
    "for old_col_name in df.columns:\n",
    "    df = df.with_column_renamed(\n",
    "        old_col_name, \n",
    "        identifier.get_unescaped_names(old_col_name)\n",
    "    )\n",
    "\n",
    "full_table_name = f\"{TEST_DATASET_DB}.{TEST_DATASET_SCHEMA}.{table_name}\"\n",
    "df.write.mode(\"ignore\").save_as_table(full_table_name)\n",
    "rows_count = session.sql(\n",
    "    f\"SELECT COUNT(*) FROM {full_table_name}\").collect()[0][0]\n",
    "\n",
    "print(f\"{full_table_name} has total {rows_count} rows.\")\n",
    "print(\"Script completes successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = session.table(\n",
    "    f\"{TEST_DATASET_DB}.{TEST_DATASET_SCHEMA}.{table_name}\")\n",
    "\n",
    "source_df = source_df.select(\n",
    "    [\n",
    "        \"TRIP_DISTANCE\", \n",
    "        \"FARE_AMOUNT\",\n",
    "        \"PASSENGER_COUNT\",\n",
    "        \"PULOCATIONID\",\n",
    "        \"DOLOCATIONID\",\n",
    "        F.cast(source_df.TPEP_PICKUP_DATETIME / 1000000, TimestampType())\n",
    "            .alias(\"PICKUP_TS\"),\n",
    "        F.cast(source_df.TPEP_DROPOFF_DATETIME / 1000000, TimestampType())\n",
    "            .alias(\"DROPOFF_TS\"),\n",
    "    ]).filter(\n",
    "        \"\"\"DROPOFF_TS >= '2016-01-01 00:00:00' \n",
    "            AND DROPOFF_TS < '2016-01-03 00:00:00'\n",
    "        \"\"\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162799",
   "metadata": {},
   "source": [
    "## Create FeatureStore Client\n",
    "\n",
    "Let's first create a feature store client.\n",
    "\n",
    "We can pass in an existing database name, or a new database will be created upon the feature store initialization. Replace `DEMO_DB` and `DEMO_SCHEMA` with your database and schema names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_DB = \"FS_TIME_SERIES_EXAMPLE\"\n",
    "DEMO_SCHEMA = \"AWESOME_FS\"\n",
    "\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {DEMO_DB}\").collect()\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DEMO_DB}\").collect()\n",
    "session.sql(f\"\"\"CREATE OR REPLACE WAREHOUSE PUBLIC WITH\n",
    "                WAREHOUSE_SIZE='XSMALL'\n",
    "            \"\"\").collect()\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=DEMO_DB, \n",
    "    name=DEMO_SCHEMA, \n",
    "    default_warehouse=\"PUBLIC\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052074",
   "metadata": {},
   "source": [
    "## Create and register new Entities\n",
    "Create entity by giving entity name and join keys. Then register it to feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70609920",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pickup = Entity(name=\"trip_pickup\", join_keys=[\"PULOCATIONID\"])\n",
    "trip_dropoff = Entity(name=\"trip_dropoff\", join_keys=[\"DOLOCATIONID\"])\n",
    "fs.register_entity(trip_pickup)\n",
    "fs.register_entity(trip_dropoff)\n",
    "fs.list_entities().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4393",
   "metadata": {},
   "source": [
    "## Define feature pipeline\n",
    "We will compute a few time series features in the pipeline here.\n",
    "Before we have *__value based range between__* in SQL, we will use a work around to mimic the calculation (NOTE: the work around won't be very accurate on computing the time series value due to missing gap filling functionality, but it should be enough for a demo purpose)\n",
    "\n",
    "We will define two feature groups:\n",
    "1. pickup features\n",
    "    - Mean fare amount over the past 2 and 5 hours\n",
    "2. dropoff features\n",
    "    - Count of trips over the past 2 and 5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71729be3",
   "metadata": {},
   "source": [
    "### This is a UDF computing time window end\n",
    "We will later turn these into built in functions for feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e403f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_STAGE = \"FS_STAGE\"\n",
    "\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {DEMO_STAGE}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\n",
    "    name=\"vec_window_end\",\n",
    "    is_permanent=True,\n",
    "    stage_location=f'@{DEMO_STAGE}',\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    replace=True,\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) \\\n",
    "        // np.timedelta64(1, 's') \\\n",
    "        // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73742b89",
   "metadata": {},
   "source": [
    "### Define feature pipeline logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# NOTE: these time window calculations are approximates and are not handling time gaps\n",
    "\n",
    "def pre_aggregate_fn(df, ts_col, group_by_cols):\n",
    "    df = df.with_column(\"WINDOW_END\", \n",
    "            F.call_udf(\"vec_window_end\", F.col(ts_col), \"15m\"))\n",
    "    df = df.group_by(group_by_cols + [\"WINDOW_END\"]).agg(\n",
    "            F.sum(\"FARE_AMOUNT\").alias(\"FARE_SUM_1_hr\"),\n",
    "            F.count(\"*\").alias(\"TRIP_COUNT_1_hr\")\n",
    "         )\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"PICKUP_TS\", [\"PULOCATIONID\"])\n",
    "    \n",
    "    window1 = Window.partition_by(\"PULOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"PULOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            \"SUM_FARE_2_hr\",\n",
    "            \"COUNT_TRIP_2hr\",\n",
    "            \"SUM_FARE_5_hr\",\n",
    "            \"COUNT_TRIP_5hr\",\n",
    "        ],\n",
    "        [\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window1),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1),\n",
    "            F.sum(\"FARE_SUM_1_hr\").over(window2),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2),\n",
    "        ]\n",
    "    ).select(\n",
    "        [\n",
    "            col(\"PULOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            (col(\"SUM_FARE_2_hr\") / col(\"COUNT_TRIP_2hr\"))\n",
    "                .alias(\"MEAN_FARE_2_hr\"),\n",
    "            (col(\"SUM_FARE_5_hr\") / col(\"COUNT_TRIP_5hr\"))\n",
    "                .alias(\"MEAN_FARE_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def dropoff_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"DROPOFF_TS\", [\"DOLOCATIONID\"])\n",
    "    window1 = Window.partition_by(\"DOLOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"DOLOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            col(\"DOLOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window1) \\\n",
    "                .alias(\"COUNT_TRIP_2_hr\"),\n",
    "            F.sum(\"TRIP_COUNT_1_hr\").over(window2) \\\n",
    "                .alias(\"COUNT_TRIP_5_hr\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "pickup_df = pickup_features_fn(source_df)\n",
    "pickup_df.show()\n",
    "\n",
    "dropoff_df = dropoff_features_fn(source_df)\n",
    "dropoff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46fa4f",
   "metadata": {},
   "source": [
    "## Create FeatureViews and materialize\n",
    "\n",
    "Once the FeatureView construction is done, we can materialize the FeatureView to the Snowflake backend and incremental maintenance will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345de0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Due to a known issue on backend pipeline creation, \n",
    "# if the source data is created right before the \n",
    "# feature pipeline, there might be a chance for \n",
    "# dataloss, so sleep for 60s for now.\n",
    "# This issue will be fixed soon in upcoming patch.\n",
    "\n",
    "import time\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fv = FeatureView(\n",
    "    name=\"trip_pickup_time_series_features\", \n",
    "    entities=[trip_pickup], \n",
    "    feature_df=pickup_df, \n",
    "    timestamp_col=\"ts\"\n",
    ")\n",
    "pickup_fv = fs.register_feature_view(\n",
    "    feature_view=pickup_fv, \n",
    "    version=\"v1\", \n",
    "    refresh_freq=\"1 minute\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8960b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_fv = FeatureView(\n",
    "    name=\"trip_dropoff_time_series_features\", \n",
    "    entities=[trip_dropoff], \n",
    "    feature_df=dropoff_df, \n",
    "    timestamp_col=\"ts\"\n",
    ")\n",
    "fs.register_feature_view(\n",
    "    feature_view=dropoff_fv, \n",
    "    version=\"v1\", \n",
    "    refresh_freq=\"1 minute\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009c81",
   "metadata": {},
   "source": [
    "## Explore FeatureViews\n",
    "We can easily discover what are the materialized FeatureViews and the corresponding features with *__fs.list_feature_views()__*. \n",
    "\n",
    "We can also apply filters based on Entity name or FeatureView names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.list_feature_views(entity_name=\"trip_pickup\") \\\n",
    "    .select([\"NAME\", \"VERSION\", \"ENTITIES\", \"FEATURE_DESC\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302cf23",
   "metadata": {},
   "source": [
    "## Generate training data and train a model\n",
    "The training data generation will lookup __point-in-time correct__ feature values and join with the spine dataframe. Optionally, you can also exclude columns in the generated dataset by providing `exclude_columns` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_df = source_df.select([\n",
    "    \"PULOCATIONID\", \n",
    "    \"DOLOCATIONID\", \n",
    "    \"PICKUP_TS\", \n",
    "    \"FARE_AMOUNT\"])\n",
    "training_data = fs.generate_dataset(\n",
    "    spine_df=spine_df,\n",
    "    features=[pickup_fv, dropoff_fv],\n",
    "    materialized_table=\"yellow_tripdata_2016_01_training_data\",\n",
    "    spine_timestamp_col=\"PICKUP_TS\",\n",
    "    spine_label_cols = [\"FARE_AMOUNT\"]\n",
    ")\n",
    "\n",
    "training_data.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bced5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "training_pd = training_data.df.to_pandas()\n",
    "X = training_pd.drop([\"FARE_AMOUNT\", \"PICKUP_TS\"], axis=1)\n",
    "y = training_pd[\"FARE_AMOUNT\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "estimator = make_pipeline(imp, LinearRegression())\n",
    "\n",
    "reg = estimator.fit(X, y)\n",
    "r2_score = reg.score(X_test, y_test)\n",
    "print(r2_score * 100,'%')\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142c25c",
   "metadata": {},
   "source": [
    "## Log model with Model Registry\n",
    "We can log the model along with its training dataset metadata with model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import model_registry, artifact\n",
    "import time\n",
    "\n",
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, \n",
    "    database_name=\"my_cool_registry\", \n",
    "    create_if_not_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_ref = registry.log_artifact(\n",
    "    artifact_type=artifact.ArtifactType.DATASET,\n",
    "    artifact_name=\"my_cool_dataset\",\n",
    "    artifact_spec=training_data.to_json(),\n",
    "    artifact_version=\"v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"my_model_{time.time()}\"\n",
    "\n",
    "model_ref = registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=\"v1\",\n",
    "    model=estimator,\n",
    "    artifacts=[artifact_ref],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0581d",
   "metadata": {},
   "source": [
    "## Restore model and predict with latest features\n",
    "Retrieve the training dataset from registry and construct dataframe of latest feature values. Then we restore the model from registry. Finally, we can predict with latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some source prediction data\n",
    "pred_df = training_pd.sample(3, random_state=996)[\n",
    "    ['PULOCATIONID', 'DOLOCATIONID', 'PICKUP_TS']]\n",
    "pred_df = session.create_dataframe(pred_df)\n",
    "pred_df = pred_df.select(\n",
    "        'PULOCATIONID', \n",
    "        'DOLOCATIONID', \n",
    "        F.cast(pred_df.PICKUP_TS / 1000000, TimestampType()) \\\n",
    "    .alias('PICKUP_TS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich source prediction data with features\n",
    "from snowflake.ml.dataset.dataset import Dataset\n",
    "\n",
    "registered_artifact = registry.get_artifact(artifact_ref.id)\n",
    "\n",
    "registered_dataset = Dataset.from_json(registered_artifact.spec, session)\n",
    "\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    spine_df=pred_df, \n",
    "    features=registered_dataset.load_features(), \n",
    "    spine_timestamp_col='PICKUP_TS'\n",
    ").drop(['PICKUP_TS']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = model_registry.ModelReference(\n",
    "    registry=registry, \n",
    "    model_name=model_name, \n",
    "    model_version=\"v1\"\n",
    ").load_model()\n",
    "\n",
    "pred = model_ref.predict(enriched_df)\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
