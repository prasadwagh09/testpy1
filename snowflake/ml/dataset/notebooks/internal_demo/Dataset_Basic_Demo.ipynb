{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e645315e-9a73-4cb0-b72e-a1ecb32abf1d",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5652801-1259-439e-8b70-df7d1995916b",
   "metadata": {},
   "source": [
    "## Import Dependencies and Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb6a13-5b93-4eff-87c6-7e65cc8398ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session, functions as F\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.ml import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c797d0-f2cd-4b17-a3ac-8445e0f83ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()\n",
    "print(session)\n",
    "\n",
    "TEST_DATASET_DB = \"DATASET_DEMO_DB\"\n",
    "TEST_DATASET_SCHEMA = \"DATASET_DEMO_SCHEMA\"\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {TEST_DATASET_DB}\").collect()\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS \n",
    "    {TEST_DATASET_DB}.{TEST_DATASET_SCHEMA}\"\"\").collect()\n",
    "session.use_database(TEST_DATASET_DB)\n",
    "session.use_schema(TEST_DATASET_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cdc84-5f2f-491d-97c6-9a0d22f294bc",
   "metadata": {},
   "source": [
    "# Prepare test data\n",
    "\n",
    "We will use the [diamond price dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) for this demo. The data can be downloaded from https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144abd6b-b56e-481f-aa07-0806c1ec32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\"\n",
    "data_pd = pd.read_csv(data_url)\n",
    "\n",
    "# Encode categorical variables: cut, color, clarity\n",
    "label_encoder = LabelEncoder()\n",
    "data_pd['cut'] = label_encoder.fit_transform(data_pd['cut'])\n",
    "data_pd['color'] = label_encoder.fit_transform(data_pd['color'])\n",
    "data_pd['clarity'] = label_encoder.fit_transform(data_pd['clarity'])\n",
    "\n",
    "# Scale numerical features: carat, x, y, z, depth, table\n",
    "numerical_features = ['carat', 'x', 'y', 'z', 'depth', 'table']\n",
    "scaler = StandardScaler()\n",
    "data_pd[numerical_features] = scaler.fit_transform(data_pd[numerical_features])\n",
    "\n",
    "df = session.create_dataframe(data_pd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931e01e-9483-44a2-b4bb-9b80719bae3a",
   "metadata": {},
   "source": [
    "Let's create a Snowflake Dataset from the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2d826-2381-40d3-9b27-b01fc6c5ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = f\"{TEST_DATASET_DB}.{TEST_DATASET_SCHEMA}.wine_data\"\n",
    "ds_version = \"v1\"\n",
    "\n",
    "session.sql(f\"DROP DATASET IF EXISTS {ds_name}\").collect()\n",
    "ds = dataset.create_from_dataframe(\n",
    "    session,\n",
    "    name=ds_name,\n",
    "    version=ds_version,\n",
    "    input_dataframe=df,\n",
    "    label_cols=[\"price\"],\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {ds.fully_qualified_name}\")\n",
    "print(f\"Selected version: {ds.selected_version.name} ({ds.selected_version})\")\n",
    "print(f\"Available versions: {ds.list_versions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd07ee9-765b-440a-9ec4-2e9a4e6adde2",
   "metadata": {},
   "source": [
    "The Dataset object includes various connectors under the `read` property which we can use to inspect or consume the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80c3e6-c888-4071-b3da-f4c5393e1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in dir(ds.read) if not f.startswith('_') and callable(getattr(ds.read, f))])\n",
    "\n",
    "print(ds.read.files())\n",
    "print(ds.read.to_pandas().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f9e7b-6428-41cc-88d3-70fd08b0d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11261b-c7ea-4bb9-b0b0-7a711dd63cec",
   "metadata": {},
   "source": [
    "We could use this dataset as-is and do any train/test split at runtime if needed. However, we might want to guarantee consistent splitting by saving the pre-split dataset as versions of our Snowflake Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25fe63-c2a4-4832-b9ac-6f4c301624e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "uniform_min, uniform_max = 1, 10\n",
    "pivot = (uniform_max - uniform_min + 1) * test_ratio\n",
    "df_aug = df.with_column(\"_UNIFORM\", F.uniform(uniform_min, uniform_max, F.random()))\n",
    "ds.create_version(\n",
    "    version=\"train\",\n",
    "    input_dataframe=df_aug.where(df_aug.col(\"_UNIFORM\") > pivot).drop(df_aug.col(\"_UNIFORM\")),\n",
    "    label_cols=[\"price\"],\n",
    ")\n",
    "ds.create_version(\n",
    "    version=\"test\",\n",
    "    input_dataframe=df_aug.where(df_aug.col(\"_UNIFORM\") <= pivot).drop(df_aug.col(\"_UNIFORM\")),\n",
    "    label_cols=[\"price\"],\n",
    ")\n",
    "\n",
    "print(ds.list_versions())\n",
    "\n",
    "train_ds = ds.select_version(\"train\")\n",
    "test_ds = ds.select_version(\"test\")\n",
    "\n",
    "print(\"train rows:\", train_ds.read.to_snowpark_dataframe().count())\n",
    "print(\"test rows:\", test_ds.read.to_snowpark_dataframe().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2ea4d-d8c1-4ef9-a7c5-78c8b3aa8779",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Let's train and evaluate a basic PyTorch model using our newly created Snowflake Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294e5ef-3d5c-467f-a044-f7710fb2b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_pd = train_ds.read.to_pandas()\n",
    "X_train = train_pd.drop(columns=[\"price\"])\n",
    "y_train = train_pd[\"price\"]\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Model\n",
    "test_pd = test_ds.read.to_pandas()\n",
    "X_test = test_pd.drop(columns=[\"price\"])\n",
    "y_test = test_pd[\"price\"]\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847e2be-fbb8-49aa-8124-433c99a42149",
   "metadata": {},
   "source": [
    "We can run this same model in a stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee64ad7-af71-495f-8746-f2fbb3b41d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_code_imports = [\n",
    "    (os.path.join(snowml_path, 'snowflake', 'ml', '_internal'), 'snowflake.ml._internal'),\n",
    "    (os.path.join(snowml_path, 'snowflake', 'ml', 'fileset'), 'snowflake.ml.fileset'),\n",
    "    (os.path.join(snowml_path, 'snowflake', 'ml', 'dataset'), 'snowflake.ml.dataset'),\n",
    "]\n",
    "for t in local_code_imports:\n",
    "    session.add_import(*t, whole_file_hash=True)\n",
    "    \n",
    "deps = [\n",
    "    \"snowflake-snowpark-python\",\n",
    "    \"snowflake-ml-python\",\n",
    "    \"cryptography\",\n",
    "]\n",
    "\n",
    "@F.sproc(session=session, packages=deps)\n",
    "def ds_sproc(session: Session) -> float:\n",
    "    train_ds = dataset.load_dataset(session, ds_name, \"train\")\n",
    "    test_ds = dataset.load_dataset(session, ds_name, \"test\")\n",
    "\n",
    "    train_pd = train_ds.read.to_pandas()\n",
    "    X_train = train_pd.drop(columns=[\"price\"])\n",
    "    y_train = train_pd[\"price\"]\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Model\n",
    "    test_pd = test_ds.read.to_pandas()\n",
    "    X_test = test_pd.drop(columns=[\"price\"])\n",
    "    y_test = test_pd[\"price\"]\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "    # Calculate the Mean Squared Error\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", ds_sproc(session))\n",
    "session.clear_imports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb0905-d544-4032-a8da-12673c202f6d",
   "metadata": {},
   "source": [
    "We can also use Dataset's connector APIs to integrate with ML frameworks like PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42996aad-95c5-44b9-b320-218e5cdd66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "class DiamondPricePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiamondPricePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, carat, cut, color, clarity, depth, table, x, y, z):\n",
    "        X = torch.cat((carat, cut, color, clarity, depth, table, x, y, z), axis=1)\n",
    "        X = self.relu(self.fc1(X))\n",
    "        X = self.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, ds: dataset.Dataset, batch_size: int = 32, num_epochs: int = 10, learning_rate: float = 1e-3):\n",
    "    model.train()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in ds.read.to_torch_datapipe(batch_size=batch_size):\n",
    "            targets = torch.from_numpy(batch.pop(\"price\")).unsqueeze(1).to(torch.float32)\n",
    "            inputs = {k:torch.from_numpy(v).unsqueeze(1) for k,v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def eval_model(model: nn.Module, ds: dataset.Dataset, batch_size: int = 32) -> float:\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in ds.read.to_torch_datapipe(batch_size=batch_size):\n",
    "            targets = torch.from_numpy(batch.pop(\"price\")).unsqueeze(1).to(torch.float32)\n",
    "            inputs = {k:torch.from_numpy(v).unsqueeze(1) for k,v in batch.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            mse += nn.functional.mse_loss(outputs, targets).item()\n",
    "    return mse\n",
    "\n",
    "model = DiamondPricePredictor()\n",
    "train_model(model, train_ds)\n",
    "eval_model(model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1009685-2c71-4db8-97a0-947f12d693d7",
   "metadata": {},
   "source": [
    "(WIP) We can pass the Datasets into SnowML modeling APIs using either Snowpark DataFrame or Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1a3bb-2747-43e4-80b1-4d847ebff347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "\n",
    "FEATURE_COLS = [\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "LABEL_COLS = [\"price\"]\n",
    "\n",
    "# Train an XGBoost model on snowflake.\n",
    "xgboost_model = XGBRegressor(\n",
    "    input_cols=FEATURE_COLS,\n",
    "    label_cols=LABEL_COLS,\n",
    ")\n",
    "\n",
    "xgboost_model.fit(train_ds.read.to_snowpark_dataframe())\n",
    "\n",
    "# Use the model to make predictions.\n",
    "predictions = xgboost_model.predict(test_ds.read.to_snowpark_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfad573-9e8b-429e-af38-c4d3316cbb5a",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "There are several features which are still on the horizon for the Dataset client API, such as:\n",
    "1. Adding multi-version Dataset support\n",
    "2. Adding exclude_cols handling to all connectors (`to_pandas()`, `to_torch_datapipe()`, etc)\n",
    "3. Consolidating FileSet functionality (reading from internal stage) into dataset.DataReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6727b-db3c-4e3e-9201-001ad5e0e98e",
   "metadata": {},
   "source": [
    "# Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fffb3-544d-4110-b941-b6230ec14604",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"DROP SCHEMA IF EXISTS {TEST_DATASET_SCHEMA}\").collect()\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {TEST_DATASET_DB}\").collect()\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
